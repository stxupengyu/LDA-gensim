{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"LDA.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOyLVmkk7QdPm7Bkp26I1au"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"rUG1T7fuHJnN"},"source":["1. 找出每个句子的主题\n","2. 相同主题出现的次数\n","3. 整个主题的可视化"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZbBkaO2wu0mY","executionInfo":{"status":"ok","timestamp":1605708740807,"user_tz":-480,"elapsed":1268,"user":{"displayName":"Expss Xu","photoUrl":"","userId":"17480852382145563764"}},"outputId":"4c74187c-093e-4816-9402-f43cb40d6119"},"source":["#挂载云盘 \n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":75,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LDfXWBTBvFLJ","executionInfo":{"status":"ok","timestamp":1605708743431,"user_tz":-480,"elapsed":1324,"user":{"displayName":"Expss Xu","photoUrl":"","userId":"17480852382145563764"}},"outputId":"7fafd2b0-6813-4bf4-93f2-c659c591be66"},"source":["#进入项目目录\n","!ls\n","import os\n","os.chdir('/content/drive/My Drive/Project/LDA项目')\n","!ls"],"execution_count":76,"outputs":[{"output_type":"stream","text":[" LDA.ipynb\t    record_topic.gsheet   内容.xlsx\n"," record_topic.csv  'stopwords (1).txt'\n"," LDA.ipynb\t    record_topic.gsheet   内容.xlsx\n"," record_topic.csv  'stopwords (1).txt'\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":845},"id":"Nw9bcsS-u-td","executionInfo":{"status":"ok","timestamp":1605708944171,"user_tz":-480,"elapsed":6704,"user":{"displayName":"Expss Xu","photoUrl":"","userId":"17480852382145563764"}},"outputId":"88869c58-5315-4825-8f1f-f8bff91b26c5"},"source":["!pip install gensim\n","import numpy as np\n","import pandas as pd\n","import re\n","from gensim import corpora, models, similarities\n","import gensim\n","#参数设置\n","num_topics = 10#主题数量\n","topn = 3#主题向量前几个\n","\n","# 用正则表达式清洗数据\n","def clean_email_text(text):\n","    text = text.replace('\\n',\" \")                        # 新行，我们是不需要的\n","    text = re.sub(r\"-\", \" \", text)                       # 把 \"-\" 的两个单词，分开。（比如：july-edu ==> july edu）\n","    text = re.sub(r\"\\d+/\\d+/\\d+\", \"\", text)              # 日期，对主体模型没什么意义\n","    text = re.sub(r\"[0-2]?[0-9]:[0-6][0-9]\", \"\", text)   # 时间，没意义\n","    text = re.sub(r\"[\\w]+@[\\.\\w]+\", \"\", text)            # 邮件地址，没意义\n","    text = re.sub(r\"/[a-zA-Z]*[:\\//\\]*[A-Za-z0-9\\-_]+\\.+[A-Za-z0-9\\.\\/%&=\\?\\-_]+/i\", \"\", text)    # 网址，没意义    \n","    # 以防还有其他除了单词以外的特殊字符（数字）等等，我们把特殊字符过滤掉\n","    # 只留下字母和空格\n","    # 再把单个字母去掉，留下单词\n","    pure_text = ''\n","    for letter in text:\n","        if letter.isalpha() or letter==' ':\n","            pure_text += letter\n","            \n","    text = ' '.join(word for word in pure_text.split() if len(word)>1)\n","    return text\n","\n","\"\"\"第一步：用正则表达式清洗数据，并去除停用词\"\"\"\n","print(\"展示下表格数据: \\n\")\n","data = pd.read_excel('内容.xlsx',header=None)\n","display(data.head(2))\n","#数据清洗\n","docs_text = data[0]\n","docs = docs_text.apply(lambda s: clean_email_text(s))  \n","# 得到所有文本的内容\n","doclist = docs.values\n","print(\"一共有\",len(doclist),\"段文本。\\n\")\n","print(\"第1段文本未清洗前的内容为: \\n\",docs_text.iloc[0],'\\n')\n","# 去除停用词，处理成gensim需要的输入格式\n","stopwords = [word.strip() for word in open('stopwords (1).txt','r').readlines()]\n","# 每一段文本都有星期和月份，这里也把他们过滤掉\n","weeks = ['monday','mon','tuesday','tues','wednesday','wed','thursday','thur','friday','fri','saturday','sat','sunday','sun']\n","months = ['jan','january','feb','february','mar','march','apr','april','may','jun','june','jul',\\\n","          'july','aug','august','sept','september','oct','october','nov','november','dec','december']\n","others = ['th']#,'covid','coronavirus'\n","stoplist = stopwords+weeks+months+others+['am','pm']\n","texts = [[word for word in doc.lower().split() if word not in stoplist] for doc in doclist]\n","print(\"第1段文本去除停用词并处理成gensim需要的格式为：\\n\",texts[0],'\\n')\n","\n","\"\"\"第二步：构建字典，将文本ID化\"\"\"\n","dictionary = corpora.Dictionary(texts)\n","corpus = [dictionary.doc2bow(text) for text in texts]\n","# 将每一篇文本ID化\n","print(\"第1段文本ID化后的结果为：\\n\",corpus[0],'\\n')\n","\n","\"\"\"第三步：训练LDA模型\"\"\"\n","lda = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics)\n","print(\"LDA模型训练完成！\\n\")\n","\n","#查看十个主题分别是什么单词\n","print(\"查看%d个主题分别是什么单词：\\n\"%num_topics)\n","for i in range(num_topics):\n","  print(lda.print_topic(i, topn=topn))\n","\n","#记录每段文本的主题\n","record_topic=[]\n","for i in range(len(doclist)):\n","  topic = lda.get_document_topics(corpus[i])\n","  record_topic.append(topic[0][0])\n","pd.DataFrame(record_topic).to_csv('record_topic.csv')\n","print(\"已记录每段文本的主题到record_topic.csv！\\n\")\n","\n","#统计每个主题出现的次数\n","times_dic={}\n","for i in range(num_topics):\n","  times_dic[i] = 0\n","for i in record_topic:\n","  times_dic[i] += 1\n","print(\"已统计每个主题出现的次数！\\n\")\n","print(times_dic)"],"execution_count":79,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (3.6.0)\n","Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (3.0.0)\n","Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.15.0)\n","Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.18.5)\n","Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.4.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.23.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2020.6.20)\n","展示下表格数据: \n","\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>CRISPR pioneer Doudna opens lab to run Covid-1...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Coronavirus Doctors Threatened to Be Fired if ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                   0\n","0  CRISPR pioneer Doudna opens lab to run Covid-1...\n","1  Coronavirus Doctors Threatened to Be Fired if ..."]},"metadata":{"tags":[]}},{"output_type":"stream","text":["一共有 1291 段文本。\n","\n","第1段文本未清洗前的内容为: \n"," CRISPR pioneer Doudna opens lab to run Covid-19 tests. #21stcenturytech #COVID19  https://statnews.com/2020/03/30/crispr-pioneer-doudna-opens-lab-to-run-covid-19-tests/… via @statnews \n","\n","第1段文本去除停用词并处理成gensim需要的格式为：\n"," ['crispr', 'pioneer', 'doudna', 'opens', 'lab', 'run', 'covid', 'tests', 'stcenturytech', 'covid', 'httpsstatnewscomcrispr', 'pioneer', 'doudna', 'opens', 'lab', 'run', 'covid', 'tests', 'via', 'statnews'] \n","\n","第1段文本ID化后的结果为：\n"," [(0, 3), (1, 1), (2, 2), (3, 1), (4, 2), (5, 2), (6, 2), (7, 2), (8, 1), (9, 1), (10, 2), (11, 1)] \n","\n","LDA模型训练完成！\n","\n","查看10个主题分别是什么单词：\n","\n","0.036*\"covid\" + 0.009*\"help\" + 0.008*\"coronavirus\"\n","0.063*\"covid\" + 0.021*\"coronavirus\" + 0.008*\"people\"\n","0.042*\"covid\" + 0.021*\"coronavirus\" + 0.008*\"new\"\n","0.040*\"covid\" + 0.014*\"coronavirus\" + 0.008*\"health\"\n","0.054*\"covid\" + 0.009*\"coronavirus\" + 0.007*\"us\"\n","0.051*\"covid\" + 0.006*\"coronavirus\" + 0.006*\"pandemic\"\n","0.047*\"covid\" + 0.024*\"coronavirus\" + 0.008*\"cases\"\n","0.052*\"covid\" + 0.023*\"coronavirus\" + 0.005*\"trump\"\n","0.034*\"covid\" + 0.008*\"coronavirus\" + 0.007*\"people\"\n","0.056*\"covid\" + 0.018*\"coronavirus\" + 0.009*\"us\"\n","已记录每段文本的主题到record_topic.csv！\n","\n","已统计每个主题出现的次数！\n","\n","{0: 261, 1: 173, 2: 84, 3: 110, 4: 132, 5: 135, 6: 126, 7: 117, 8: 47, 9: 106}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8wBKyA3pRhPk"},"source":[""],"execution_count":null,"outputs":[]}]}